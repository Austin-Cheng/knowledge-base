
什么是智能？真正的智能，应该是具备推理能力，能验证假设，且能为未来做准备的。合适的测评帮助理解智能的水平，就像人类的IQ测试。

# 一、LLM

## GPT系列

1. GPT-4
2. GPT-3.5-turbo

## Llama系列

Meta公司，原名Facebook，分布的。2023-07-19，发布免费可商用的Llama2。

1. Llama2-70B
2. Llama2-13B
3. Llama2-7B
4. Llama 1

## Claude系列

Anthropic公司研制的

1. Claude-v1
2. Claude-instant-v1

## Vicuna系列

由多家研究机构合作推出的一个开源大语言模型，其研究团队来自于UC Berkeley、CMU、斯坦福、US San Dego和MBZUAI。该系列的模型是基于Meta LLaMA在SharedGPT开放数据集上微调得到。因此，模型本身受限于LLaMA的非商用限制以及OpenAI对ChatGPT共享数据集的限制。官方宣称该模型水平接近ChatGPT，并且超过其它开源的模型。

Vicuna官网：https://lmsys.org/blog/2023-03-30-vicuna/
Vicuna在线使用：https://chat.lmsys.org/?arena

1. Vicuna-33B，模型卡：https://www.datalearner.com/ai-models/pretrained-models/Vicuna-33B
2. Vicuna-13B，模型卡：https://www.datalearner.com/ai-models/pretrained-models/Vicuna-13B
3. Vicuna-13B，模型卡：https://www.datalearner.com/ai-models/pretrained-models/Vicuna-7B


# 二、LLM评测数据集和基准

LLM模型被设计来解决各种任务。LLM评测数据集，用于测试和对比不同的LLM模型在各种任务上的效果。比如，GLUE和SuperGLUE，旨在模拟真实世界的场景，覆盖各种任务，比如文本分类、机器翻译、阅读理解、对话生成。

评测基准：
1. chatbot arena，大模型竞技平台，伯克利大学，评测平台：https://lmsys.org/
评测榜单：https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
2. MT-Bench，主要评估多轮问答的能力。
3. HELM
4. Big-Bench
5. MME
6. KoLA
7. DynaBench
8. MMLU
9. GLUE-X，旨在评估NLP模型在OOD场景下的robustness。
10. PromptBench，用于增强LLM微调的方法。
11. PandaLM

## 针对特定的下游任务的

1. MultiMedQA
2. C-Eval
3. M3Exam
4. GAOKAO_Bench
5. SOCKET
6. MATH
7. APPS
8. CUAD
9. CVALUES

# 三、LLM评估

# 四、比赛

# 五、行业

# 六、资料
